# üöÄ‚ú® Plataforma de Inteligencia de Mercado Laboral: LatAm Insights

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python&logoColor=white)](https://www.python.org/)
[![Built with Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://streamlit.io/)
[![Powered by Supabase](https://img.shields.io/badge/Supabase-Powered-green?logo=supabase&logoColor=white)](https://supabase.com/)
[![Google Gemini API](https://img.shields.io/badge/Google_Gemini-API-purple?logo=google-gemini&logoColor=white)](https://ai.google.dev/)

---

## üåü Descripci√≥n General del Proyecto

Sum√©rgete en el coraz√≥n del mercado laboral de Latinoam√©rica con la **Plataforma de Inteligencia de Mercado Laboral**. Esta soluci√≥n integral y automatizada est√° dise√±ada para **rastrear, procesar, analizar y visualizar** las tendencias de empleo digital m√°s relevantes, con un enfoque principal en **Latinoam√©rica** y la capacidad de expansi√≥n global.

Construida en **Python**, nuestra plataforma de vanguardia integra:
*   **Scrapy** para un web scraping potente y eficiente.
*   Un sofisticado **Pipeline de ETL** (Extracci√≥n, Transformaci√≥n, Carga) para asegurar la m√°xima calidad y coherencia de los datos.
*   **Supabase** como una base de datos robusta y escalable en la nube.
*   Capacidades de **An√°lisis de Tendencias** para descifrar patrones ocultos.
*   Un **Generador de Reportes inteligente** potenciado por la **IA de Google Gemini** para insights accionables.
*   Todo presentado en un **Dashboard interactivo y din√°mico** creado con **Streamlit**.

Nuestro objetivo es simple: **democratizar el acceso a la inteligencia del mercado laboral.** Proporcionamos una visi√≥n cristalina y casi en tiempo real sobre:
*   üìà Las **habilidades t√©cnicas m√°s codiciadas**.
*   üöÄ Los **roles con mayor demanda y crecimiento**.
*   üè¢ Las **empresas l√≠deres en contrataci√≥n**.
*   üåç Las **tendencias emergentes por sector** (FinTech, EdTech, HealthTech, ¬°y m√°s!).

Esta valiosa informaci√≥n empodera a profesionales, reclutadores, instituciones educativas y empresas para tomar decisiones estrat√©gicas basadas en datos s√≥lidos.

---

## ‚ú® Caracter√≠sticas Estelares

Explora la potencia de nuestra plataforma a trav√©s de sus componentes clave:

### üï∏Ô∏è Web Scraping Multi-Plataforma (`scrapers/`)
*   **Adaptabilidad:** Spiders especializados para extraer vacantes de **LinkedIn** y **Computrabajo**, con expansi√≥n flexible a otras plataformas.
*   **B√∫squeda Inteligente:** Configura tus b√∫squedas por palabras clave (roles, habilidades, sectores) y ubicaciones geogr√°ficas precisas (continentes, pa√≠ses).
*   **Anti-Bloqueo Avanzado:** Implementa rotaci√≥n de User-Agents, retrasos aleatorios y AutoThrottle para una recolecci√≥n de datos sigilosa y efectiva.
*   **Exploraci√≥n Profunda:** Navegaci√≥n autom√°tica por m√∫ltiples p√°ginas de resultados para una cobertura exhaustiva.

### üßπ Pipeline ETL de Vanguardia (`etl/` & `scrapers/pipelines.py`)
Un sistema cuidadosamente dise√±ado para transformar datos crudos en informaci√≥n valiosa:
*   **Limpieza de Datos (`etl/cleaners.py`):** Elimina etiquetas HTML, espacios redundantes y caracteres especiales. Los datos se pulen para ser legibles y coherentes. Generaci√≥n de IDs √∫nicos (`job_id`) robustos.
*   **Normalizaci√≥n Est√°ndar (`etl/normalizers.py`):** Estandariza la jerarqu√≠a profesional (Junior, Mid, Senior, Lead, Executive), el tipo de contrato (Full-time, Remote, Hybrid) y la clasificaci√≥n de roles (Data Science & ML, Software Development).
*   **Enriquecimiento Corporativo (`etl/enrichment.py`):** Utiliza heur√≠sticas para inferir informaci√≥n clave de empresas: tama√±o, industria, pa√≠s de sede y tipo de organizaci√≥n, aportando contexto invaluable.
*   **Extracci√≥n de Habilidades (`etl/skill_extractor.py`):** Identifica y clasifica autom√°ticamente habilidades t√©cnicas cruciales (ej. `Python`, `AWS`, `Machine Learning`) de las descripciones de empleo.
*   **Clasificaci√≥n Sectorial (`etl/sector_classifier.py`):** Asigna cada vacante a un sector industrial espec√≠fico (Fintech, Edtech, etc.) bas√°ndose en un an√°lisis inteligente de palabras clave.

### üíæ Persistencia en Supabase (`database/supabase_client.py`)
*   **Almacenamiento Confiable:** Tu centro de datos en la nube, impulsado por PostgreSQL a trav√©s de Supabase, garantizando escalabilidad y seguridad.
*   **Deduplicaci√≥n Inteligente:** Mecanismos `upsert` que evitan registros duplicados y mantienen la informaci√≥n fresca y actualizada.
*   **Modelo Relacional:** Organiza eficientemente vacantes, habilidades, compa√±√≠as y tendencias en un esquema de base de datos interconectado.

### üìä An√°lisis de Tendencias Detallado (`analysis/trend_analyzer.py`)
*   **M√©tricas Esenciales:** Calcula y almacena las **habilidades m√°s demandadas**, las **habilidades con mayor crecimiento**, los **roles m√°s buscados** y la **distribuci√≥n de vacantes por sector**.
*   **Visi√≥n Temporal:** Realiza an√°lisis comparativos entre diferentes periodos de tiempo, revelando la evoluci√≥n del mercado.
*   **Historial de Tendencias:** Los resultados se persisten en Supabase, construyendo un valioso archivo hist√≥rico de la evoluci√≥n del mercado laboral.

### üß† Generaci√≥n de Insights con IA (`analysis/report_generator.py`)
*   **Inteligencia Artificial con Google Gemini:** Aprovecha el poder de los modelos generativos de Google para transformar datos en narrativas coherentes.
*   **Reportes Ejecutivos:** Genera res√∫menes diarios concisos que resaltan las tendencias clave, como los roles emergentes o las empresas m√°s activas en contrataci√≥n.

### üåê Dashboard Interactivo (`dashboard.py`)
*   **Interfaz Amigable:** Un panel de control intuitivo y visualmente atractivo construido con Streamlit, accesible desde tu navegador.
*   **Control Total:** Ejecuta procesos de scraping y an√°lisis de tendencias directamente desde la interfaz, sin necesidad de comandos complejos.
*   **Filtros Din√°micos:** Explora los datos con filtros por continente, pa√≠s y rango de fechas para una personalizaci√≥n total.

### ‚öôÔ∏è Gesti√≥n Centralizada de Configuraci√≥n (`config/`)
*   **`config.yaml`:** Un archivo YAML f√°cil de editar que centraliza todos los par√°metros clave: roles de b√∫squeda, palabras clave para sectores y una biblioteca exhaustiva de habilidades t√©cnicas.
*   **`geo.py`:** Define la geograf√≠a del proyecto, con mapeos de continentes, pa√≠ses y configuraciones espec√≠ficas para los filtros de fecha de cada plataforma.

### ‚è∞ Programaci√≥n de Tareas (`scheduler.py`)
*   **Automatizaci√≥n Sencilla:** Un script ligero que utiliza la librer√≠a `schedule` para automatizar la ejecuci√≥n peri√≥dica de tareas esenciales como el scraping y el an√°lisis, manteniendo tus datos siempre al d√≠a.

---

## üìÇ Estructura del Proyecto

Una visi√≥n r√°pida de c√≥mo est√° organizado este ingenioso sistema:

```
Proyecto Automatizado/
‚îú‚îÄ‚îÄ analysis/                     # M√≥dulos para an√°lisis y generaci√≥n de reportes IA.
‚îÇ   ‚îú‚îÄ‚îÄ report_generator.py       # Crea res√∫menes inteligentes con Google Gemini.
‚îÇ   ‚îî‚îÄ‚îÄ trend_analyzer.py         # Descubre tendencias de habilidades, roles y sectores.
‚îú‚îÄ‚îÄ config/                       # Archivos esenciales de configuraci√≥n.
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml               # Define tu universo de b√∫squeda: roles, sectores y habilidades clave.
‚îÇ   ‚îî‚îÄ‚îÄ geo.py                    # Datos geogr√°ficos y mapeos para una b√∫squeda precisa.
‚îú‚îÄ‚îÄ database/                     # La capa de interacci√≥n con tu base de datos Supabase.
‚îÇ   ‚îî‚îÄ‚îÄ supabase_client.py        # Gestiona todas las operaciones CRUD y `upsert` con Supabase.
‚îú‚îÄ‚îÄ dashboard.py                  # Tu centro de mando visual: el Dashboard interactivo de Streamlit.
‚îú‚îÄ‚îÄ etl/                          # El coraz√≥n de la transformaci√≥n de datos.
‚îÇ   ‚îú‚îÄ‚îÄ cleaners.py               # Limpia y estandariza el texto de las vacantes.
‚îÇ   ‚îú‚îÄ‚îÄ enrichment.py             # Enriquecimiento heur√≠stico para datos de compa√±√≠as.
‚îÇ   ‚îú‚îÄ‚îÄ normalizers.py            # Normaliza campos clave como antig√ºedad y tipo de trabajo.
‚îÇ   ‚îú‚îÄ‚îÄ sector_classifier.py      # Clasifica vacantes por sector industrial.
‚îÇ   ‚îî‚îÄ‚îÄ skill_extractor.py        # Extrae y categoriza habilidades t√©cnicas autom√°ticamente.
‚îú‚îÄ‚îÄ scrapers/                     # Donde nacen los datos: tus herramientas de scraping.
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # Paquete Python para los scrapers.
‚îÇ   ‚îú‚îÄ‚îÄ items.py                  # Define la estructura de datos para cada vacante.
‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py            # Estrategias anti-bloqueo como rotaci√≥n de User-Agents.
‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py              # La secuencia de procesamiento de datos antes de Supabase.
‚îÇ   ‚îî‚îÄ‚îÄ spiders/                  # Los "bots" que rastrean las plataformas de empleo.
‚îÇ       ‚îú‚îÄ‚îÄ computrabajo_spider.py# Spider dedicado a Computrabajo.
‚îÇ       ‚îú‚îÄ‚îÄ linkedin_spider.py    # Spider dedicado a LinkedIn.
‚îÇ       ‚îî‚îÄ‚îÄ company_enrichment_spider.py # Un concepto para futuras expansiones de enriquecimiento.
‚îú‚îÄ‚îÄ tests/                        # Garantizando la calidad: pruebas unitarias del proyecto.
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_etl_components.py    # Pruebas para tus m√≥dulos ETL cr√≠ticos.
‚îú‚îÄ‚îÄ main.py                       # El director de orquesta: punto de entrada para todas las operaciones.
‚îú‚îÄ‚îÄ README.md                     # ¬°Este mismo archivo! Tu gu√≠a principal.
‚îú‚îÄ‚îÄ requirements.txt              # La lista de ingredientes: todas las dependencias de Python.
‚îú‚îÄ‚îÄ scheduler.py                  # El automatizador: para ejecutar tareas programadas.
‚îî‚îÄ‚îÄ ver_dashboard.bat             # El atajo: script de Windows para lanzar el dashboard al instante.
```

---

## üõ†Ô∏è Configuraci√≥n y Requisitos Previos: Gu√≠a Completa de Instalaci√≥n

¬°Prepara tu entorno de desarrollo para esta emocionante aventura!

### 1. Requisitos del Sistema üíª

Aseg√∫rate de que tu sistema operativo tenga instalados los siguientes elementos:

*   **Python 3.9 o superior:**
    *   ‚¨áÔ∏è Descarga desde: [python.org](https://www.python.org/downloads/)
    *   ‚ú® **Verificaci√≥n:** Abre tu terminal (o S√≠mbolo del Sistema/PowerShell en Windows) y escribe `python --version` (o `python3 --version`). Deber√≠as ver una versi√≥n como `Python 3.9.x` o superior.
*   **Git:**
    *   ‚¨áÔ∏è Descarga desde: [git-scm.com](https://git-scm.com/downloads)
    *   ‚ú® **Verificaci√≥n:** En tu terminal, escribe `git --version`.

### 2. Clonar el Repositorio del Proyecto üì•

1.  Abre tu terminal (o Git Bash en Windows).
2.  Navega al directorio donde deseas guardar el proyecto.
3.  Ejecuta el siguiente comando para descargar una copia local del c√≥digo:

    ```bash
    git clone https://github.com/tu-usuario/Proyecto-Automatizado.git # ‚ö†Ô∏è ¬°IMPORTANTE! Reemplaza "tu-usuario/Proyecto-Automatizado.git" con la URL REAL de tu repositorio de GitHub.
    cd Proyecto-Automatizado
    ```
    Este comando crear√° una nueva carpeta `Proyecto-Automatizado` y te posicionar√° dentro de ella.

### 3. Configurar Supabase como tu Base de Datos üîó

Supabase act√∫a como el back-end de nuestra plataforma, almacenando todos los datos recopilados y analizados.

#### A. Crear un Nuevo Proyecto en Supabase üöÄ
1.  **Visita Supabase:** Dir√≠gete a [supabase.com](https://supabase.com/).
2.  **Reg√≠strate/Inicia Sesi√≥n:** Crea una cuenta o inicia sesi√≥n en tu panel de control.
3.  **Nuevo Proyecto:** Haz clic en el bot√≥n "New project" para comenzar la creaci√≥n.
4.  **Detalles del Proyecto:**
    *   **Name:** Elige un nombre descriptivo (ej. "JobMarketIntelligence").
    *   **Database Password:** **Crea y anota una contrase√±a segura.** ¬°Es crucial para tu base de datos!
    *   **Region:** Selecciona la regi√≥n geogr√°fica m√°s cercana a tu ubicaci√≥n (o donde planeas desplegar el proyecto) para optimizar el rendimiento.
5.  **Obt√©n tus Credenciales API:** Una vez que Supabase haya terminado de provisionar tu proyecto (puede tardar unos minutos), navega a "Project Settings" ‚û°Ô∏è "API" en el panel lateral izquierdo.
    *   **Project URL:** Copia la URL de tu proyecto. Tendr√° un formato similar a `https://[TU_PROYECTO_ID].supabase.co`.
    *   **Anon Key (`anon public`):** Copia esta clave. Se utiliza principalmente para operaciones de **lectura** desde el dashboard de Streamlit.
    *   **Service Role Key (`service_role secret`):** Copia esta clave. Posee permisos de **administrador completo** y se utilizar√° para las operaciones de **escritura (upsert)** y **borrado** desde el backend del scraper y el an√°lisis. **¬°MANT√âN ESTA CLAVE BAJO EXTREMA SEGURIDAD Y NUNCA LA EXPONGAS EN EL C√ìDIGO DEL LADO DEL CLIENTE!**

#### B. Aplicar el Esquema SQL para Crear las Tablas üß±
Necesitas definir la estructura de las tablas en tu base de datos Supabase para que el proyecto pueda almacenar los datos correctamente.

1.  **Localiza el Archivo SQL:** El script SQL necesario para crear las tablas (`companies`, `jobs`, `skills`, `trends`) se encuentra en el archivo **`SQL_PARA_SUPABASE.sql`** en la ra√≠z del directorio de tu proyecto local (`Proyecto Automatizado/`).
    *   **Descripci√≥n del Esquema:** Este archivo est√° meticulosamente dise√±ado. Define las relaciones entre tablas (con claves for√°neas como `company_id` y `job_id`), establece restricciones de unicidad para evitar duplicados y crea √≠ndices para acelerar las consultas, garantizando la integridad y eficiencia de tus datos.
2.  **Accede al SQL Editor de Supabase:** En el panel de control de tu proyecto Supabase, haz clic en "SQL Editor" en la barra lateral izquierda.
3.  **Crea un Nuevo Query:** Haz clic en el bot√≥n "New query".
4.  **Copia y Pega el Contenido:** Abre el archivo `SQL_PARA_SUPABASE.sql` desde tu proyecto local con cualquier editor de texto, copia **todo su contenido** y p√©galo en el √°rea de texto del editor de queries de Supabase.
5.  **Ejecuta el Query:** Haz clic en el bot√≥n "Run" (generalmente un tri√°ngulo ‚ñ∂Ô∏è) para ejecutar el script SQL. En pocos segundos, todas las tablas necesarias se crear√°n en tu base de datos.

### 4. Configurar Variables de Entorno (`.env`) üîë

Para que tu proyecto Python pueda acceder a Supabase y a la API de Google Gemini, debes configurar tus credenciales como variables de entorno.

1.  **Crear el Archivo `.env`:** En el directorio ra√≠z de tu proyecto (`Proyecto Automatizado/`), crea un nuevo archivo y n√≥mbralo exactamente `.env` (sin ninguna extensi√≥n visible).
2.  **A√±adir Credenciales:** Copia y pega el siguiente contenido en el archivo `.env`. **¬°REEMPLAZA los valores `[TU_...]` con tus credenciales reales** obtenidas de Supabase y Google!

    ```dotenv
    # .env
    # --------------------------------------------------------------------------------------
    # Configuraciones de Supabase
    # SUPABASE_URL: La URL de tu proyecto Supabase (ej. https://abcdefghijk.supabase.co)
    SUPABASE_URL="https://[TU_PROYECTO_ID].supabase.co"

    # SUPABASE_KEY: Tu "anon public" key. Se usa principalmente para operaciones de LECTURA
    #               desde el dashboard de Streamlit, o para escritura si RLS est√° configurado.
    SUPABASE_KEY="[TU_SUPABASE_ANON_KEY]" 

    # SUPABASE_SERVICE_KEY: Tu "service_role secret" key. Tiene permisos de ADMINISTRADOR.
    #                       Se usa para operaciones de ESCRITURA (upsert) y BORRADO de datos
    #                       desde los pipelines de Scrapy y el script de limpieza.
    #                       ¬°MANT√âN ESTA CLAVE EXTREMADAMENTE SEGURA Y NO LA EXPONGAS EN EL CLIENTE!
    SUPABASE_SERVICE_KEY="[TU_SUPABASE_SERVICE_ROLE_KEY]"

    # --------------------------------------------------------------------------------------
    # Configuraciones de Google Gemini
    # GEMINI_API_KEY: Tu clave de API para Google Gemini. Necesaria para el generador de reportes de IA.
    #                 Obt√©n una clave en https://ai.google.dev/
    GEMINI_API_KEY="[TU_GOOGLE_GEMINI_API_KEY]"
    ```
    **‚ö†Ô∏è ADVERTENCIA DE SEGURIDAD:** El archivo `.env` **NUNCA** debe ser compartido p√∫blicamente (ej. subido a GitHub). Aseg√∫rate de que tu archivo `.gitignore` incluya `.env` para evitar esto.

### 5. Instalar las Dependencias de Python üì¶

Con tu terminal a√∫n en el directorio `Proyecto Automatizado/`, instala todas las bibliotecas de Python que el proyecto requiere.

1.  Ejecuta el siguiente comando:
    ```bash
    pip install -r requirements.txt
    ```
    Este comando leer√° la lista de paquetes en `requirements.txt` y los instalar√° autom√°ticamente. Este proceso puede tardar unos minutos en completarse.

---

## üöÄ Uso de la Plataforma: Gu√≠a Detallada de Operaci√≥n

¬°Est√°s listo para darle vida a tu Plataforma de Inteligencia de Mercado Laboral! Sigue estos pasos para comenzar a recopilar datos, analizarlos y visualizarlos.

### 1. Ejecutar los Scrapers para Recopilar Vacantes üï∑Ô∏è

Puedes iniciar el proceso de extracci√≥n de datos de dos maneras: interactivamente a trav√©s de la terminal o mediante argumentos de l√≠nea de comandos para una automatizaci√≥n precisa.

#### A. Modo Interactivo (¬°Recomendado para las primeras exploraciones!)
Este modo te guiar√° con preguntas sencillas para configurar tu sesi√≥n de scraping.
1.  Abre tu terminal en el directorio `Proyecto Automatizado/`.
2.  Ejecuta el script principal sin ning√∫n argumento:
    ```bash
    python main.py
    ```
3.  **Sigue las Instrucciones:** El script te solicitar√° la siguiente informaci√≥n:
    *   **Selecci√≥n de Scrapers:** Te mostrar√° una lista de spiders disponibles (ej. `linkedin`, `computrabajo`). Ingresa los n√∫meros correspondientes separados por comas (ej. `1,2` para ambos, o `1` para solo LinkedIn).
    *   **Selecci√≥n de Continente:** Escoge el continente de tu inter√©s de la lista presentada.
    *   **Selecci√≥n de Pa√≠s:** Dentro del continente elegido, podr√°s optar por un pa√≠s espec√≠fico o seleccionar "Todos los pa√≠ses" para abarcar todas las ubicaciones en ese continente.
    *   **Rango de Fechas (Opcional):** Se te pedir√° ingresar una "Fecha de inicio" y una "Fecha de fin" (formato `YYYY-MM-DD`). Si dejas estos campos en blanco, el scraper intentar√° obtener todas las vacantes disponibles sin un filtro de fecha estricto desde la plataforma de origen (el filtrado por fecha preciso se realizar√° en el pipeline de ETL si los spiders no lo soportan nativamente en la URL).
    *   **N√∫mero M√°ximo de Vacantes:** Define el n√∫mero l√≠mite de vacantes que **cada scraper** intentar√° obtener en esta ejecuci√≥n.

#### B. Modo L√≠nea de Comandos (¬°Ideal para automatizaci√≥n y scripts!)
Si ya conoces tus par√°metros de b√∫squeda, puedes pasarlos directamente al script, perfecto para integraciones o ejecuciones repetitivas.
1.  Abre tu terminal en el directorio `Proyecto Automatizado/`.
2.  Ejecuta el script `main.py` con los argumentos apropiados. Aqu√≠ tienes un ejemplo exhaustivo:
    ```bash
    python main.py \
      --spiders linkedin,computrabajo \
      --continent Latam \
      --country "Todos los Pa√≠ses" \
      --start_date 2024-03-01 \
      --end_date 2024-03-31 \
      --max_jobs 200
    ```
    *   **`--spiders [spider1,spider2,...]`**: Especifica qu√© spiders ejecutar, separados por comas (ej. `linkedin,computrabajo`).
    *   **`--continent [NombreContinente]`**: El continente objetivo (ej. `Latam`, `Europa`, `Norte America`).
    *   **`--country [NombrePais | "Todos los Pa√≠ses"]`**: Un pa√≠s espec√≠fico (ej. `Mexico`, `Argentina`). Si eliges `"Todos los Pa√≠ses"`, rastrear√° todas las ubicaciones dentro del `--continent` especificado.
    *   **`--start_date [YYYY-MM-DD]`**: La fecha de inicio m√≠nima para las vacantes publicadas.
    *   **`--end_date [YYYY-MM-DD]`**: La fecha de fin m√°xima para las vacantes publicadas.
    *   **`--max_jobs [Numero]`**: El n√∫mero m√°ximo de vacantes que **cada spider** intentar√° raspar en esta ejecuci√≥n.

### 2. Ejecutar el An√°lisis de Tendencias üìà

Despu√©s de haber recopilado una cantidad significativa de vacantes, el siguiente paso es ejecutar el m√≥dulo de an√°lisis de tendencias. Este proceso calcular√° m√©tricas clave y almacenar√° los insights resultantes en tu base de datos Supabase.

1.  Abre tu terminal en el directorio `Proyecto Automatizado/`.
2.  Ejecuta el script principal con el argumento `--analyze-trends`:
    ```bash
    python main.py --analyze-trends
    ```
    Este comando calcular√° las tendencias m√°s demandadas (habilidades, roles, sectores) utilizando los datos disponibles y las almacenar√° en la tabla `trends` de tu base de datos Supabase, asoci√°ndolas a la fecha actual.

3.  **An√°lisis para una Fecha Espec√≠fica (Opcional):**
    Si necesitas realizar un an√°lisis retrospectivo para una fecha en particular, puedes especificarla:
    ```bash
    python main.py --analyze-trends --analysis-date 2024-03-15
    ```

### 3. Iniciar el Dashboard de Streamlit üìä

El Dashboard de Streamlit es tu centro de comando visual, donde podr√°s explorar los datos recopilados, visualizar gr√°ficos, m√©tricas y los insights generados por la IA.

1.  Abre tu terminal en el directorio `Proyecto Automatizado/`.
2.  Ejecuta el siguiente comando para lanzar la aplicaci√≥n de Streamlit:
    ```bash
    streamlit run dashboard.py
    ```
3.  **Accede al Dashboard:** Streamlit iniciar√° un servidor web local y, en la mayor√≠a de los casos, abrir√° autom√°ticamente el dashboard en tu navegador web predeterminado (generalmente en `http://localhost:8501`).
    *   **Para Usuarios de Windows:** Para mayor comodidad, puedes simplemente hacer doble clic en el archivo `ver_dashboard.bat` (si existe en la ra√≠z de tu proyecto) o ejecutarlo desde el S√≠mbolo del Sistema. Este script est√° dise√±ado para lanzar el dashboard r√°pidamente.
4.  **Explora e Interact√∫a:** Una vez en el dashboard, te encontrar√°s con:
    *   **Panel de Control (Barra Lateral):** Aqu√≠ tienes acceso directo para ejecutar nuevos procesos de scraping, iniciar el an√°lisis de tendencias, y, si es necesario, limpiar la base de datos.
    *   **Filtros Din√°micos:** Utiliza los filtros en la barra lateral (por continente, pa√≠s, rango de fechas) para afinar los datos que se muestran en los gr√°ficos y tablas.
    *   **Visualizaciones Impactantes:** Observa gr√°ficos interactivos que ilustran las habilidades m√°s demandadas, los roles predominantes, la distribuci√≥n por sector y los valiosos reportes de IA generados por Google Gemini.

### 4. Programar Tareas Peri√≥dicas (Opcional - Uso Avanzado) ‚è∞

El script `scheduler.py` te permite automatizar la ejecuci√≥n de las tareas de scraping y an√°lisis a intervalos regulares, manteniendo tu plataforma actualizada sin intervenci√≥n manual.

1.  **Localiza y Adapta `scheduler.py`:** El archivo `scheduler.py` se encuentra en la ra√≠z de tu proyecto. √Åbrelo con tu editor de texto preferido.
    *   **Entiende la Estructura:** Este archivo contiene funciones Python (ej. `job_scrape_latam_linkedin()`, `job_analyze_trends()`) que envuelven las llamadas a `main.py` mediante `subprocess.run()`.
    *   **Modifica la L√≥gica:** Ajusta estas funciones para que reflejen tus necesidades espec√≠ficas de automatizaci√≥n (ej. qu√© spiders ejecutar, qu√© pa√≠ses, qu√© rangos de fechas). Aseg√∫rate de que los argumentos pasados a `subprocess.run` concuerden con los que `main.py` espera en modo CLI.
    *   **Define la Frecuencia:** Modifica las l√≠neas `schedule.every().day.at("02:00").do(...)` para establecer la periodicidad y la hora de ejecuci√≥n de cada tarea. Puedes usar `every().hour`, `every().monday`, `every(5).minutes`, etc.
2.  **Ejecutar el Scheduler:**
    Para que las tareas programadas se ejecuten, el script `scheduler.py` debe permanecer activo en segundo plano.
    *   **En Desarrollo/Pruebas:** Puedes ejecutarlo directamente desde tu terminal:
        ```bash
        python scheduler.py
        ```
        Mant√©n esta terminal abierta. Puedes detener el scheduler en cualquier momento presionando `Ctrl+C`.
    *   **En Producci√≥n (Recomendado):** Para un despliegue robusto y fiable, se aconseja ejecutar el scheduler como un proceso en segundo plano que sea gestionado por un sistema. Herramientas comunes para esto incluyen `nohup` (en Linux/macOS), `systemd` (Linux), `supervisor` o `pm2`.
        *   **Ejemplo con `nohup` (Linux/macOS):**
            ```bash
            nohup python scheduler.py > scheduler_output.log 2>&1 &
            ```
            Este comando ejecutar√° el scheduler de forma persistente en segundo plano. Su salida (logs) se redirigir√° a `scheduler_output.log`, y tu terminal quedar√° libre para otros usos.

### 5. Limpiar la Base de Datos (¬°üö® ADVERTENCIA: ACCI√ìN IRREVERSIBLE! üö®) üóëÔ∏è

Existe una funcionalidad en el dashboard de Streamlit para eliminar **todos los datos** de las tablas `jobs`, `skills`, `companies` y `trends`. Utiliza esta opci√≥n con extrema precauci√≥n.

1.  **Inicia el Dashboard:** Aseg√∫rate de que tu dashboard de Streamlit est√© activo y funcionando (`streamlit run dashboard.py`).
2.  **Navega al Panel de Control:** En la barra lateral izquierda del dashboard, busca la secci√≥n "‚ö†Ô∏è Mantenimiento de Datos".
3.  **Haz Clic en "Limpiar Base de Datos":** Al activar este bot√≥n, aparecer√° una ventana de confirmaci√≥n con una advertencia clara.
4.  **Confirma la Acci√≥n:** **LEE CUIDADOSAMENTE LA ADVERTENCIA.** Si est√°s absolutamente seguro de proceder, haz clic en "S√≠, Eliminar Datos".
    *   **¬°Importante!** Esta operaci√≥n requiere que la `SUPABASE_SERVICE_KEY` configurada en tu archivo `.env` tenga los **permisos expl√≠citos de `delete`** en Supabase. Si la limpieza falla, revisa los logs en la terminal de Streamlit para identificar posibles errores de permisos o problemas de conexi√≥n a la base de datos.

---

## üåê Tecnolog√≠as y Librer√≠as Utilizadas üöÄ

Este proyecto es una muestra del poder del ecosistema Python, utilizando una selecci√≥n de herramientas y bibliotecas de vanguardia:

*   **Lenguaje de Programaci√≥n:** `Python` (3.9+) üêç
*   **Web Scraping y Automatizaci√≥n:**
    *   `Scrapy`: El framework fundamental para el rastreo web de alto rendimiento.
    *   `beautifulsoup4`: Una librer√≠a vers√°til para parsear HTML de manera eficiente (utilizada en `TextCleaner`).
    *   `selenium` & `webdriver-manager`: Para interactuar con navegadores web reales (√∫til para contenido din√°mico de JS, aunque no siempre activo en todos los spiders).
    *   `requests`: Para realizar solicitudes HTTP sencillas y directas.
    *   `lxml`: Un potente parser de XML/HTML optimizado para velocidad.
    *   `fake-useragent`: Para generar encabezados `User-Agent` realistas y aleatorios, mejorando la resistencia a bloqueos.
*   **Base de Datos y Persistencia:**
    *   `supabase-py`: El cliente oficial de Python para interactuar sin problemas con tu base de datos Supabase.
    *   `python-dotenv`: Gestiona y carga tus variables de entorno desde el archivo `.env` de forma segura.
*   **Procesamiento y An√°lisis de Datos:**
    *   `pandas`: La piedra angular para la manipulaci√≥n, limpieza y an√°lisis de datos tabulares.
    *   `numpy`: Proporciona soporte para operaciones num√©ricas y arrays de alto rendimiento.
    *   `pyyaml`: Para la f√°cil lectura y escritura de tus archivos de configuraci√≥n YAML.
    *   `python-dateutil`: Un m√≥dulo poderoso para el parsing y manipulaci√≥n inteligente de fechas y horas.
    *   `scikit-learn`: (Potencialmente para futuras extensiones de ML, no expl√≠citamente usado para modelos en el MVP de ETL).
*   **Visualizaci√≥n y Dashboards Interactivas:**
    *   `streamlit`: El innovador framework que transforma scripts de Python en elegantes aplicaciones web interactivas y dashboards.
    *   `plotly`: Librer√≠a de gr√°ficos interactivos de √∫ltima generaci√≥n para visualizaciones ricas y din√°micas.
    *   `altair`: (Posiblemente utilizado para algunas visualizaciones declarativas, aunque Plotly es el principal motor gr√°fico).
*   **Inteligencia Artificial y Modelos Generativos:**
    *   `google-generativeai`: El SDK de Python para integrar las capacidades de los modelos de IA de Google Gemini.
*   **Automatizaci√≥n y Scheduling:**
    *   `schedule`: Una librer√≠a simple y eficaz para programar la ejecuci√≥n de tareas recurrentes directamente en Python.
*   **Herramientas de Desarrollo y Testing:**
    *   `flake8`: Para asegurar la conformidad con el estilo de c√≥digo PEP8 y mantener un c√≥digo limpio.
    *   `pytest`: Un framework de pruebas robusto para escribir tests unitarios eficientes y escalables.
    *   `openpyxl` & `xlsxwriter`: (Si se implementan funcionalidades avanzadas de exportaci√≥n de reportes a Excel).

---

## ü§ù Contribuciones üí°

¬°Valoramos inmensamente cada contribuci√≥n a este proyecto! Tu apoyo es fundamental para hacerlo crecer y mejorarlo. Si tienes ideas, detectas un error o deseas a√±adir una nueva funcionalidad, te animamos a participar.

Para contribuir, sigue los pasos de un flujo de trabajo est√°ndar de GitHub:

1.  **Haz un Fork:** Dir√≠gete al repositorio original en GitHub y haz clic en el bot√≥n "Fork" para crear una copia personal en tu cuenta.
2.  **Clona tu Fork:** Descarga la copia de tu repositorio a tu m√°quina local.
3.  **Crea una Rama Nueva:** Antes de realizar cualquier cambio, crea una rama espec√≠fica para tu contribuci√≥n. Esto mantiene el historial de cambios organizado.
    ```bash
    git checkout -b feature/tu-nueva-funcionalidad # Para a√±adir caracter√≠sticas
    git checkout -b fix/solucion-del-problema       # Para corregir errores
    ```
4.  **Realiza tus Cambios:** Implementa tus mejoras o correcciones. Esfu√©rzate por seguir las buenas pr√°cticas de codificaci√≥n y mantener la consistencia del estilo del proyecto.
5.  **A√±ade Pruebas (¬°Si Aplica!):** Si est√°s introduciendo nuevas funcionalidades o corrigiendo un bug, por favor, incluye pruebas unitarias relevantes en el directorio `tests/`. Esto garantiza que tus cambios no introduzcan nuevos problemas y que la funcionalidad sea robusta.
6.  **Commitea tus Cambios:** Escribe mensajes de commit claros, concisos y descriptivos que expliquen qu√© cambios has realizado y por qu√©.
    ```bash
    git commit -am 'feat: Integrar un nuevo scraper para la plataforma X'
    git commit -am 'fix: Mejorar el parsing de salarios en el Computrabajo spider'
    ```
7.  **Sincroniza y Haz Push:** Antes de enviar tu Pull Request, aseg√∫rate de que tu rama est√© actualizada con la versi√≥n m√°s reciente del repositorio principal para evitar conflictos. Luego, sube tus cambios a tu fork:
    ```bash
    git pull origin main # Sincroniza con la rama principal (main)
    git push origin feature/tu-nueva-funcionalidad
    ```
8.  **Abre un Pull Request (PR):** Finalmente, ve a la p√°gina de tu fork en GitHub. Ver√°s una opci√≥n para "Open a Pull Request". Proporciona una descripci√≥n detallada de tus cambios, el problema que resuelven o la funcionalidad que a√±aden. ¬°Estaremos encantados de revisarlo!

---

## üìÑ Licencia ‚öñÔ∏è

Este proyecto est√° distribuido bajo la **Licencia MIT**. Esto te otorga una gran libertad para usar, copiar, modificar, fusionar, publicar, distribuir, sublicenciar y/o vender copias del software.

La √∫nica condici√≥n es que se incluya el aviso de derechos de autor original y este aviso de licencia en todas las copias o partes sustanciales del Software.

Para leer el texto completo de la licencia, por favor, consulta el archivo `LICENSE` ubicado en la ra√≠z del repositorio.

---
**Desarrollado con ‚ù§Ô∏è para empoderar el mercado laboral.**