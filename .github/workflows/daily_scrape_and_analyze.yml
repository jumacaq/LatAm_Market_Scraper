# .github/workflows/daily_scrape_and_analyze.yml
name: Daily LatAm Job Scrape and Analysis

on:
  # 1. Ejecutar de forma programada (cron)
  # Este cron se ejecuta todos los días a las 6:00 AM UTC.
  # Puedes ajustar la hora según tus necesidades.
  # (Ej: '0 9 * * *' para 9 AM UTC, '0 12 * * *' para 12 PM UTC)
  schedule:
    - cron: '0 6 * * *' 
  
  # 2. Permite ejecutar el workflow manualmente desde la interfaz de GitHub
  workflow_dispatch: 

  # 3. Ejecutar en cada push a la rama 'main' (útil para probar cambios en el workflow)
  push:
    branches:
      - main

jobs:
  scrape_and_analyze_job:
    runs-on: ubuntu-latest # Se ejecuta en un entorno Linux virtual

    defaults:
      run:
        # Importante: Establece el directorio de trabajo para todos los comandos 'run'.
        # Esto es crucial porque tu proyecto está anidado.
        # Asegúrate de que esta ruta sea correcta dentro de tu repositorio.
        # Por ejemplo, si tu repo es "mi-repo" y dentro tienes "ProyectoUndio - ... /job-market-intelligence",
        # la ruta debe ser desde la raíz del repo hasta "job-market-intelligence".
        working-directory: ProyectoUndio -ProyectoFinal/ProyectoUltimo.Final1 - copia/PrroyectoFUNCIONANDOULTIMO2025-11-26/job-market-intelligence

    steps:
      - name: Checkout repository code
        uses: actions/checkout@v4 # Obtiene tu código del repositorio

      - name: Set up Python environment
        uses: actions/setup-python@v5 # Configura Python
        with:
          python-version: '3.9' # Asegúrate de que esta versión coincida con tus necesidades

      - name: Install dependencies
        run: pip install -r requirements.txt # Instala todas las librerías desde requirements.txt

      - name: Run Job Scrapers (LinkedIn, Computrabajo)
        env:
          # Carga tus credenciales de Supabase y Gemini desde los Secrets de GitHub
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }} # Clave anon pública también puede ser útil
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          echo "Executing scrapers for LatAm countries..."
          python main.py --spiders linkedin,computrabajo --continent Latam --country "Todos los Países" --max_jobs 200
          echo "Scraping complete."
        
      - name: Run Trend Analysis
        env:
          # El análisis de tendencias también necesita acceso a Supabase
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }} # En caso de que se necesite para futuros análisis
        run: |
          echo "Executing trend analysis..."
          python main.py --analyze-trends
          echo "Trend analysis complete."